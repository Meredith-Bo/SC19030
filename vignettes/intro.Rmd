---
title: "Introduction to SC19030"
author: "Xiaolin Bo"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to SC19030}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

__SC19030__ is a simple R package developed to acheieve density estimation with unimodal constraint for the 'Statistical Computing' course. Five functions are considered, namely, _umd_ (achieving unimodal density estimation using Berstein polynomial), _mOpt.CN_ (calculating the optimal number of weights). _maxWeight_(determining the index of the maximum weights), _constraintMat_(function to generate the constraint matrix), and _solveWeights_(function to calculate the weights vector under the constraint).For these five functions, _umd_ is the main function, and the other four functions are what we need to invoke in _umd_.For each function, R versions are produced.


The source R code for _mOpt.CN_ is as follows:
```{r,eval=FALSE}
mOpt.CN <- function(tData, L){
  n <- length(tData)
  m <- floor(  n^(1/3) ) - 1  
  logratio <- 1
  while( logratio < sqrt(n) ){
    m <- m+1
    B <- NULL
    for(k in 1:m){
      B <- cbind(B, pbeta(tData, shape1=k, shape2=m-k+1))
    }
    Dmat <- t(B) %*% L %*% B
    spec <- eigen(Dmat, symmetric=TRUE)
    d <- spec$values
    min.eigenValue <- max( min(d), 0 )
    max.eigenValue <- max(d)
    logratio <- log10(max.eigenValue) - log10(min.eigenValue)		
  }	
  m-1
}
```

_mOpt.CN_ regurns the number of the beta distribution mixture, which is necessary for the unimodal density estimation.

The source R code for _maxWeight_ is as follows:
```{r,eval=FALSE}
maxWeight <- function(m, Fn, lower, upper){
  #find max of weights
  maxplace <- which.max( Fn( ((1:m)/m)) - 
                           Fn( ((0:(m-1))/m) )  )
  maxplace
}
```

_maxWeight_ returns the index of the maximum weight of the beta mixture, which could be treated as the primary component.

The source R code for _constraintMat_ is as follows:
```{r,eval=FALSE}
constraintMat <- function( m, maxplace){
  A <- suppressWarnings(   
    rbind( rep(1,m), diag(rep(1,m)), 
           matrix( rep( c(-1,1, rep(0,m - 1)) , maxplace-1), maxplace-1, m, byrow=TRUE), 
           matrix( rep( c( rep (0,maxplace-1),1,-1,rep(0, m-maxplace)),m-maxplace),
                   m-maxplace,m,byrow=TRUE))
  )
  Amat <- t(A)
  Amat
}
```

_constraintMat_ returns a matrix which plays a role of constraint in the unimodal density estimation.

The source R code for _solveWeights_ is as follows:
```{r,eval=TRUE}
library(quadprog)
solveWeights <- function(m, Fn, lower, upper, Dmat, dvec){	
  max.place <- maxWeight(m, Fn, lower, upper)  
  Amat <- constraintMat(m, max.place)	
  bvec<-c(1,rep(0,2*m-1))
  w.hat<- solve.QP(Dmat,dvec,Amat,bvec,meq=1)$solution  
  max0 <- function(x){max(x,0)}
  w.hat <- sapply( w.hat, max0)
  wsum <- sum(w.hat)
  w.hat <- w.hat / wsum
  w.hat
}	
```

_solveWeights_ returns a vector of weights of beta mixture.


##Package "quadprog" is necessary in the _umd_ function.
The source R code for _umd_ is as follows:
```{r,eval=FALSE}
library(quadprog)
#Function to determine the optimal number of weights
#using the CN criterion
mOpt.CN <- function(tData, L){
  
  #set starting point for m
  n <- length(tData)
  m <- floor(  n^(1/3) ) - 1  
  
  #set start value for while loop
  logratio <- 1
  
  while( logratio < sqrt(n) ){
    m <- m+1
    
    #construct D matrix using m value 
    B <- NULL
    for(k in 1:m){
      B <- cbind(B, pbeta(tData, shape1=k, shape2=m-k+1))
    }
    Dmat <- t(B) %*% L %*% B
    
    #take spectral decomposition to find eigenvalues
    spec <- eigen(Dmat, symmetric=TRUE)
    d <- spec$values
    min.eigenValue <- max( min(d), 0 )
    max.eigenValue <- max(d)
    
    logratio <- log10(max.eigenValue) - log10(min.eigenValue)		
  }	
  m-1 #return number of weights
}

#Function to determine the index of the maximum weight
maxWeight <- function(m, Fn, lower, upper){
  #find max of weights
  maxplace <- which.max( Fn( ((1:m)/m)) - 
                           Fn( ((0:(m-1))/m) )  )
  maxplace
}

#Function to generate the constraint matrix 
constraintMat <- function( m, maxplace){
  A <- suppressWarnings(   
    rbind( rep(1,m), diag(rep(1,m)), 
           matrix( rep( c(-1,1, rep(0,m - 1)) , maxplace-1), maxplace-1, m, byrow=TRUE), 
           matrix( rep( c( rep (0,maxplace-1),1,-1,rep(0, m-maxplace)),m-maxplace),
                   m-maxplace,m,byrow=TRUE))
  )
  Amat <- t(A)
  Amat
}

#Function to solve for the weight vector	
solveWeights <- function(m, Fn, lower, upper, Dmat, dvec){	
  #find the location of the maximum weight
  max.place <- maxWeight(m, Fn, lower, upper)  ## return index
  #make the constraint matrix
  Amat <- constraintMat(m, max.place)	
  #make bvec vector of constraints
  bvec=c(1,rep(0,2*m-1))
  #find weights using solve.QP function
  w.hat = solve.QP(Dmat,dvec,Amat,bvec,meq=1)$solution   ## solve a quadratic programming problem
  #function to find max of an element and 0
  max0 <- function(x){max(x,0)}
  #make sure no weights are < 0
  w.hat <- sapply( w.hat, max0)
  #make sure the weights sum to 1
  wsum <- sum(w.hat)
  w.hat <- w.hat / wsum
  w.hat
}	
##################################################################################

#The main function which the user will call
umd <- function(data, bound.type="sd", fix.lower=NA, fix.upper=NA, crit="CN", 
                m=NA, warning=TRUE){
  #Tranform the data to the [0,1] support
  
  #delta definition
  n <- length(data)
  delta <- sd(data)/ sqrt(n)
  sort.data <- sort(data)	
  
  #find lower and upper values for transformation
  #use given lower and upper values if given
  if(bound.type=="sd"){
    lower.calc <- min(data) - delta
  }else if(bound.type=="Carv"){
    lower.calc <- sort.data[1] - 
      (sort.data[2] - sort.data[1])/( (1-0.05)^-2 - 1)
  }	
  
  if( is.na(fix.lower) == TRUE){
    lower <- lower.calc
  }else{ 
    #if fix.lower is given take the larger of 
    #fix.lower and min data point with correction
    lower <- max( lower.calc, fix.lower ) 
    #warn if fix.lower is less than the data
    if( min(data) < fix.lower && warning == TRUE){
      cat("WARNING: data contains values less than fix.lower \n")
    }	
  }
  
  #upper bound		
  if(bound.type=="sd"){
    upper.calc <- max(data) + delta
  }else if(bound.type=="Carv"){
    upper.calc <- sort.data[n] + 
      (sort.data[n] - sort.data[n-1])/( (1-0.05)^-2 - 1)
  }	
  
  if( is.na(fix.upper) == TRUE){
    upper <- upper.calc	
  }else{ 
    #if fix.upper is given, take the smaller of 
    #fix.upper and max data point with correction
    upper <- min( upper.calc, fix.upper ) 
    #warn if gUpper is more than data
    if( max(data) > fix.upper && warning==TRUE){
      cat("WARNING: data contains values greater than fix.upper \n")
    }	
  }
  
  #Make the transformed data set
  tdata <- (data-lower)/(upper-lower)	
  
  #Construct the L matrix and make vector of ecdf values
  Fn <- ecdf(tdata)
  ep <- 3/( 8*length(data) )	
  ecdf.vec <- Fn(tdata)	
  
  #construct L
  L <- diag( n / ( ( ecdf.vec + ep)*(1 + ep - ecdf.vec ) ) )
  
  
  #Find the optimal number of weights, depends on the given crit
  #then return the desired functions
  if( crit == "CN"){
    mOpt <- mOpt.CN(tdata, L)
    
    if( is.na(m) == FALSE){
      
      if( mOpt < m && warning==TRUE){
        cat("WARNING: given number of weights is larger than optimal number,\n"
            , "\t \t optimal number of weights =", mOpt, "\n")
      }
      if( mOpt > m && warning==TRUE){
        cat("WARNING: given number of weights is less than optimal number,\n"
            ,"\t \t optimal number of weights =", mOpt, "\n")
      }
      
      #make B, Dmat, and dvec
      B <- NULL
      for(k in 1:m){
        B <- cbind(B, pbeta(tdata, shape1=k, shape2=m-k+1))
      }
      Dmat <- t(B) %*% L %*% B
      dvec <- t(B)%*% L %*% ecdf.vec
      
      #Make sure there are no zero eigen values	
      spec <- eigen(Dmat, symmetric=TRUE)
      d <- spec$values
      Q <- spec$vectors
      
      #find which values are < 10e-6, and set to smallest
      #eigen value >= 10e-6
      if( min(d) < 10e-6){
        tooSmall <- which( d < 10e-6)	
        d[tooSmall] <- d[ min(tooSmall) - 1]
        #Recreate pos. def. Dmat matrix 
        Dmat <- Q %*% diag(d) %*% t(Q)				
      }	
      
    } #if no m provided then use the optimal m
    else{ m <- mOpt 				
    #make B, Dmat, and dvec
    B <- NULL
    for(k in 1:m){
      B <- cbind(B, pbeta(tdata, shape1=k, shape2=m-k+1))
    }
    Dmat <- t(B) %*% L %*% B
    dvec <- t(B)%*% L %*% ecdf.vec
    
    #Make sure there are no zero eigen values	
    spec <- eigen(Dmat, symmetric=TRUE)
    d <- spec$values
    Q <- spec$vectors
    
    #find which values are < 10e-6, and set to smallest
    #eigen value >= 10e-6
    if( min(d) < 10e-6){
      tooSmall <- which( d < 10e-6)	
      d[tooSmall] <- d[ min(tooSmall) - 1]
      #Recreate pos. def. Dmat matrix 
      Dmat <- Q %*% diag(d) %*% t(Q)
    }
    }
    
    #Solve for the weights
    weights <- solveWeights(m,Fn,lower,upper,Dmat,dvec)
    
    #use the weights to create the 4 distribution functions
    #pdf
    dumd = function(x){ 
      mix.pdf = function(x){ifelse( (x-lower)*(upper-x) >= 0, 
                                    sum(weights*dbeta((x-lower)/(upper-lower),1:m,m:1))/(upper-lower),0)}
      sapply(x, mix.pdf)
    }
    #cdf
    pumd = function(x){ 
      mix.cdf= function(j){sum(weights*pbeta( (j-lower)/(upper-lower), 1:m, m:1)) }
      sapply(x, mix.cdf)
    }
    #rand generator
    rumd = function(n=1){
      rsample = function(){
        k <- sample(1:m,size=1,prob=weights)
        return(  lower+( rbeta(1, k, m-k+1)*(upper-lower) )   )  
      }
      replicate(n, rsample() )
    }
    #quantile function
    qumd = function(q){
      mix.quantile = function(q){ g = function(x){ pumd(x)-q }
      return( uniroot(g, interval=c(lower,upper) )$root ) 
      }
      sapply(q, mix.quantile)
    }
    
    #Return list (ends function) 	
    return(list(weights=weights,m.hat=m,dumd=dumd,pumd=pumd,rumd=rumd,qumd=qumd,
                lower=lower, upper=upper))
  }
  else if(crit %in% c("AIC","BIC")) {
    
    #Find Optimal number of weights using AIC and BIC criterion	
    m.test <- seq( 2, ceiling(n/log(n)) )
    
    #lists and vectors to hold the values in the loop
    AICs <- rep(0, length(m.test) )
    BICs <- rep(0, length(m.test) )
    weights.list <- list()
    
    #loop through m values and calculate the AIC
    for( i in 1:length(m.test)){
      
      #make B, Dmat, and dvec for this m
      B <- NULL
      for(k in 1:m.test[i]){
        B <- cbind(B, pbeta(tdata, shape1=k, shape2=m.test[i]-k+1))
      }
      Dmat <- t(B) %*% L %*% B
      dvec <- t(B)%*% L %*% ecdf.vec
      
      #Make sure there are no zero eigen values	
      spec <- eigen(Dmat, symmetric=TRUE)
      d <- spec$values
      Q <- spec$vectors
      if( min(d) < 10e-6){
        tooSmall <- which( d < 10e-6)	
        d[tooSmall] <- d[ min(tooSmall) - 1]
        #Recreate pos. def. Dmat matrix 
        Dmat <- Q %*% diag(d) %*% t(Q)		
      }
      
      #Calculate weights
      weights.list[[i]] <- solveWeights(m.test[i],Fn,lower,upper,Dmat,dvec)
      
      #Make the pdf
      dumd.now = function(x){ 
        mix.pdf = function(x){ifelse( (x-lower)*(upper-x) >= 0, 
                                      sum(weights.list[[i]]*dbeta((x-lower)/(upper-lower),
                                                                  1:m.test[i],m.test[i]:1))/(upper-lower),0)}
        sapply(x, mix.pdf)
      }
      
      #Calculate AIC and BIC
      AICs[i]<- -2*sum( log(dumd.now(data))) + 2*(m.test[i]-1)
      BICs[i]<- -2*sum( log(dumd.now(data))) + log(n)*(m.test[i]-1)	
    }
    
    #Which AIC or BIC is the smallest
    if(crit == "AIC"){
      best.m <- which.min(AICs)
    }else if(crit == "BIC"){
      best.m <- which.min(BICs)
    }
    
    #Set weights and mOpt 
    weights <- weights.list[[best.m]]
    mOpt <- m.test[best.m]
    
    #see if user supplied their own m value
    if( is.na(m) == FALSE){
      
      if( mOpt < m && warning==TRUE){
        cat("WARNING: given number of weights is larger than optimal number,\n"
            , "\t \t optimal number of weights =", mOpt, "\n")
      }
      if( mOpt > m && warning==TRUE){
        cat("WARNING: given number of weights is less than optimal number,\n"
            ,"\t \t optimal number of weights =", mOpt, "\n")
      }	
      
      #make B, Dmat, and dvec
      B <- NULL
      for(k in 1:m){
        B <- cbind(B, pbeta(tdata, shape1=k, shape2=m-k+1))
      }
      Dmat <- t(B) %*% L %*% B
      dvec <- t(B)%*% L %*% ecdf.vec
      
      #Make sure there are no zero eigen values	
      spec <- eigen(Dmat, symmetric=TRUE)
      d <- spec$values
      Q <- spec$vectors
      
      #find which values are < 10e-6, and set to smallest
      #eigen value >= 10e-6
      if( min(d) < 10e-6){
        tooSmall <- which( d < 10e-6)	
        d[tooSmall] <- d[ min(tooSmall) - 1]
        #Recreate pos. def. Dmat matrix 
        Dmat <- Q %*% diag(d) %*% t(Q)				
      }
      
      #Calculate weights	
      weights <- solveWeights(m,Fn,lower,upper,Dmat,dvec)						
    }
    else{
      #if no m is provided then just set m to mOpt
      #weights are already set
      m <- mOpt
    }	 
    
    #Calculate items to return to user:
    #pdf
    dumd = function(x){ 
      mix.pdf = function(x){ifelse( (x-lower)*(upper-x) >= 0, 
                                    sum(weights*dbeta((x-lower)/(upper-lower),1:m,m:1))/(upper-lower),0)}
      sapply(x, mix.pdf)
    }
    #cdf
    pumd = function(x){ 
      mix.cdf= function(j){sum(weights*pbeta( (j-lower)/(upper-lower), 1:m, m:1)) }
      sapply(x, mix.cdf)
    }
    #rand generator
    rumd = function(n=1){
      rsample = function(){
        k <- sample(1:m,size=1,prob=weights)
        return(  lower+( rbeta(1, k, m-k+1)*(upper-lower) )   )  
      }
      replicate(n, rsample() )
    }
    #quantile function
    qumd = function(q){
      mix.quantile = function(q){ g = function(x){ pumd(x)-q }
      return( uniroot(g, interval=c(lower,upper) )$root ) 
      }
      sapply(q, mix.quantile)
    }	
    
    #Return list (ends function) 	
    return(list(weights=weights,m.hat=m,dumd=dumd,pumd=pumd,rumd=rumd,qumd=qumd,
                lower=lower, upper=upper))
  }	
  #end the function
}
```

## An example to illustrate how _umd_ works to achieve unimodal density estimation:

```{r,eval=TRUE}
library(SC19030)
h10<-c(30.4, 18.3, 12.7, 12.8, 15.2, 7.5, 14.9, 16.7, 8.9, 15.8, 10.1, 9.5, 21.6, 6.6, 10.9, 10.6, 10.2, 14.4, 18.8, 15.9, 16.2, 15.6, 14.4, 15.2, 9.7, 17.6, 9.5, 14.3, 10.5, 15.1, 10.8, 13.9,17.4, 8.1, 9.9, 6.5, 19.7, 8.2, 13.9, 14.3, 13.6, 17.3, 16.6, 7.5, 8.1, 17.0, 14.9, 5.6, 14.5, 14.6, 15.3, 13.6, 10.6, 14.5, 13.4, 18.9, 8.1)

## mOpt.CN
mOpt.CN(h10,diag(seq(1,57,by=1),nrow=57,ncol=57))
## maxWeight
n <- length(h10)
delta <- sd(h10)/ sqrt(n)
lower<- min(h10) - delta
upper<- max(h10) + delta
tdata <- (h10-lower)/(upper-lower)	
Fn<-ecdf(tdata)
print(maxWeight(7,Fn))
## constraintMat
print(constraintMat(7,3))
## umd
umd.h10.CN<-umd(h10, bound.type="sd", crit="CN")
umd.h10.CN
y<-seq(0,40,by=0.05)
hist(h10,breaks=9,freq=FALSE,main="Density estimates of windspeed",xlim=c(0,40),ylim=c(0,0.15),xlab="Windspeed",ylab="Density")
lines(y,umd.h10.CN$dumd(y),type='l',col='red')
lines(density(h10),col='black')
```

The results show that using density estimatin with Gaussian kernel, there would be a spurious peak in the tail, which is not what we want, and using _umd_ function achieve the density estimation with unimodal constranit.

Here comes the homeworks during this semester:

HW1: Use knitr to produce at lease 3 examples(texts,figures,tables)
```{r}
library (knitr)
library(ggplot2)
library(xtable)
library(highlight)

data(pressure)  
pressure   

res<-lm(pressure$pressure~pressure$temperature) 
print(res)
summary(res)
co <- summary(res)$coefficients
co

plot(res)

ggplot(pressure,aes(x=temperature,y=pressure))+geom_point()   

ggplot(pressure,aes(x=temperature,y=pressure))+geom_point()+geom_smooth()  

ggplot(pressure,aes(x=temperature,y=pressure))+geom_point(colour="red")+geom_smooth()  


knitr::kable(pressure)  
knitr::kable(co)  
```

HW2: Generate random samples from Rayleigh Distribution, normal location mixture and Wishart distribution.
```{r}
library(knitr)
set.seed(123)
n<-1000
u<-runif(n)
x<-sqrt(-8*log(u))
hist(x,ylim=c(0,0.35),prob=TRUE,main=expression(f(x)==x/4*exp(-x^2/8)))
y<-seq(0,8,0.01)
lines(y,y/4*exp(-y^2/8),col="blue")
```

```{r}
n<-1000
set.seed(1234)
x1<-rnorm(n,0,1)
x2<-rnorm(n,3,1)
u<-runif(n)
r1<-as.integer(u>0.25)
z1<-r1*x1+(1-r1)*x2
r2<-as.integer(u>0.5)
z2<-r2*x1+(1-r2)*x2
r3<-as.integer(u>0.75)
z3<-r3*x1+(1-r3)*x2
r4<-as.integer(u>0.9)
z4<-r4*x1+(1-r4)*x2
## par(mfrow=c(2,2))
hist(z1,ylim=c(0,0.3),prob=TRUE,main="mixture for p1=0.75")
lines(density(z1),col="blue")
hist(z2,ylim=c(0,0.2),prob=TRUE,main="mixture for p1=0.5")
lines(density(z2),col="blue")
hist(z3,ylim=c(0,0.3),prob=TRUE,main="mixture for p1=0.25")
lines(density(z3),col="blue")
hist(z4,ylim=c(0,0.4),prob=TRUE,main="mixture for p1=0.1")
lines(density(z4),col="blue")

set.seed(1000)
r5<-as.integer(u>0.55)
z5<-r5*x1+(1-r5)*x2
r6<-as.integer(u>0.45)
z6<-r6*x1+(1-r6)*x2
## par(mfrow=c(1,2))
hist(z5,ylim=c(0,0.25),prob=TRUE,main="mixture for p1=0.45")
lines(density(z5),col="blue")
hist(z6,ylim=c(0,0.25),prob=TRUE,main="mixture for p1=0.55")
lines(density(z6),col="blue")
```

```{r}
set.seed(12345)
n<-15
d<-10
x<-rnorm(d*d/2,0,1) 

##generate random sample from chisquare
y<-numeric(d)
for (i in 1:d){
  X<-matrix(rnorm(n-i+1),1,n-i+1)^2
  y[i]<-rowSums(X)
}

#fill the matrix with normal randoms and chi-square randoms
k<-1 
T<-matrix(nrow=d,ncol=d)
for (i in 1:d){
  for(j in 1:d){
    if (j<i){
      T[i,j]<-x[k]
      k<-k+1}
    if(j==i){T[i,j]<-y[i]}
    if(j>i){T[i,j]<-0}
  }
}

A<-T%*%t(T)  ##A has a Wd(Id,n) distribution
B<-runif(d)  ##generate Sigma 
C<-B%*%t(B)
V<-C+diag(d)
L<-chol(V)  ##V is the covariance matrix of the multiple normal distribution
W<-L%*%A%*%t(L) ##generate random sample of Wishart distribution
print(W)
```

HW3: Compute a Monte Carlo estimate, Monte Carlo integration and obtain the stratified importance sampling estimate.
```{r}
library(knitr)
library(xtable)
set.seed(100)
m<-1e4
x<-runif(m,min=0,max=pi/3)
theta.hat<-mean(sin(x))*pi/3
print(c(theta.hat,1-cos(pi/3)))

set.seed(123)
R<-10000
n<-1000
MC1<-numeric(n)
for(i in 1:n){
u <- runif(R/2)
MC1[i]<-mean(c(exp(-u)/(1+u^2),exp(u-1)/(1+(1-u)^2)))
}
print(cbind(mean(MC1),var(MC1)))

## algorithm without antithetic variables
set.seed(1234)
MC2<-numeric(n)
n<-1000
for (i in 1:n) {
  u <- runif(R)
  MC2[i]<- mean(exp(-u)/(1+u^2))
}
print(var(MC2))

(var(MC2)-var(MC1))/var(MC2)

## stratified importance sampling
set.seed(100)
M<-1e4
k<-5
r<-M/k  #replicates per stratum
N<-50 # number of times to repeat the estimation
g<-function(x){exp(-x)/(1+x^2)*(x>0)*(x<1)}
fg<-function(x,b1,b2){
  ifelse((x>=b1&x<b2),g(x)/(exp(-x)/(1-exp(-1))),NA)}
u<-c(0,0.2,0.4,0.6,0.8,1) 
a<-(-1)*log(1-u*(1-1/exp(1))) ##generate quantiles
estimate<-matrix(0,N,2)
T2<-numeric(k)
y<-numeric(M)
for(i in 1:N){
  estimate[i,1]<-mean(g(runif(M)))
  for(j in 1:k){
    y<-fg(runif(M),a[j],a[j+1])
    y1<-na.omit(y) ##delete NA
    T2[j]<-mean(y1)
  }
    estimate[i,2]<-mean(T2)
}
apply(estimate,2,mean)
apply(estimate,2,var)


## f3, inverse transform method
set.seed(101)
u<-runif(M)
theta.hat<-se<-numeric(1)
x<- (-1)*log(1-u*(1-exp(-1)))
fg<-g(x)/(exp(-x)/(1-exp(-1)))
theta.hat<-mean(fg)
se<-sd(fg)
knitr::kable(cbind(theta.hat,se))
var(estimate[,2])
var.importance<-var(fg)/M
(var.importance-var(estimate[,2]))/var.importance
```

HW4: Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples, and estimate the 0.025,0.05,0.95,and 0.975 quantiles of the skewness $\sqrt{b_{1}}$ under normality by a Monte Carlo experiment.
```{r}
library(knitr)
##compute the t-interval 
set.seed(1)
n<-20
alpha<-.05
CI<- replicate(1000,expr={
  x<-rchisq(n,2)
 c(mean(x)-qt(1-alpha/2,df=n-1)*sd(x)/sqrt(n),
  mean(x)+qt(1-alpha/2,df=n-1)*sd(x)/sqrt(n))
})
CI<-as.matrix(CI,ncol=2)
##calculate the coverage using Monte Carlo
coverage<-0
for(i in 1:1000){
    if(CI[1,i]<2&CI[2,i]>2) coverage<-coverage+1}
print(coverage)
##estimate the coverage rate
print(coverage/1000)

n<-20
alpha<-.05
UCL<-replicate(1000,expr={
x<-rchisq(n,df=2)
(n-1)*var(x)/qchisq(alpha,df=n-1)
})
sum(UCL>4)
mean(UCL>4)

set.seed(3)
n<-10000
m<-1000
## computes the sample skewness coeff
sk<-function(x){
  xbar<-mean(x)
  m3<-mean((x-xbar)^3)
  m2<-mean((x-xbar)^2)
  return(m3/m2^1.5)
}
## Monte Carlo Experiments for skewness
skewness<-numeric(n)
for(i in 1:n){
  x<-rnorm(m)
  skewness[i]<-sk(x)
}
## the output of quantiles
xq<-cbind(as.numeric(quantile(skewness,0.025)),as.numeric(quantile(skewness,0.05)),as.numeric(quantile(skewness,0.95)),as.numeric(quantile(skewness,0.975)))
knitr::kable(xq,formate="html",col.names=c("2.5% quantile","5% quantile","95% quantile","97.5% quantile"))

se<-function(q,xq){
  sqrt(q*(1-q)/n/dnorm(xq,0,sqrt(6/m))^2)
}
knitr::kable(cbind(se(0.025,xq[1]),se(0.05,xq[2]),se(0.95,xq[3]),se(0.975,xq[4])),formate="html",col.names=c("se of 2.5%","se of 5%","se of 95%","se of 97.5%"))

## quantiles of large approximation
knitr::kable(cbind(qnorm(0.025,0,sqrt(6/m)),qnorm(0.05,0,sqrt(6/m)),qnorm(0.95,0,sqrt(6/m)),qnorm(0.975,0,sqrt(6/m))),formate = "html",col.names = c("2.5%","5%","95%","97.5%"))
## comparison of two methods
knitr::kable(cbind(as.numeric(quantile(skewness,0.025)/qnorm(0.025,0,sqrt(6/m))),as.numeric(quantile(skewness,0.05)/qnorm(0.05,0,sqrt(6/m))),as.numeric(quantile(skewness,0.95)/qnorm(0.95,0,sqrt(6/m))),as.numeric(quantile(skewness,0.975)/qnorm(0.975,0,sqrt(6/m)))),formate="html",col.names=c("ratio of 2.5%","ratio of 5%","ratio of 95%","ratio of 97.5%"))
```

HW5: Estimate the power of the skewness test of normality and use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level $\alpha$.
```{r}
library(knitr)
set.seed(0)
n<-50
cv<-qnorm(0.975,0,sqrt(6*(n-2)/(n+1)/(n+3)))
## compute the sample skewness coeff
sk<-function(x){
xbar<-mean(x)
m3<-mean((x-xbar)^3)
m2<-mean((x-xbar)^2)
return(m3/m2^1.5)
}

## compute power of beta(alpha,alpha) distribution,alpha from 1 to 10.
m<-1000
sktests<-matrix(nrow=10,ncol=m)
pwr.beta<-numeric(10)
for(i in 1:10){
for(j in 1:m){
  x<-rbeta(n,i,i)
  sktests[i,j]<-as.integer(abs(sk(x))>=cv)
}
  pwr.beta[i]<-mean(sktests[i,])
}
print(pwr.beta)

## compute power of t(3) distribution
sktests1<-matrix(nrow=10,ncol=m)
pwr.t<-numeric(10)
for(i in 1:10){
for(j in 1:m){
  x<-rt(n,i)
  sktests[i,j]<-as.integer(abs(sk(x))>=cv)
}
  pwr.t[i]<-mean(sktests[i,])
}
print(pwr.t)
plot(pwr.beta,xlab = "alpha",ylab = "power",type="b")
plot(pwr.t,xlab = "alpha",ylab = "power",type="b")

m<-10000
n<-50
alpha<-0.05
cv1<-qt(1-alpha/2,n-1)
test1<-numeric(m)
for(i in 1:m){
x1<-rchisq(n,1)
T1<-(mean(x1)-1)/sd(x1)*sqrt(n)
test1[i]<-as.integer(abs(T1)>=cv1)
}

## t-test for U(0,2) distribution
test2<-numeric(m)
for(j in 1:m){
  x2<-runif(n,0,2)
  T2<-(mean(x2)-1)/sd(x2)*sqrt(n)
test2[j]<-as.integer(abs(T2)>=cv1)
}

## t-test for Exp(1) distribution
test3<-numeric(m)
for(k in 1:m){
  x3<-rexp(n,1)
   T3<-(mean(x3)-1)/sd(x3)*sqrt(n)
test3[k]<-as.integer(abs(T3)>=cv1)
}
knitr::kable(cbind(mean(test1),mean(test2),mean(test3)),formate="html",col.names=c("t1e of chisquare(1) ","t1e of U(0,2)","t1e of Exp(1)"))
```

HW6: Use a panel display to display the sactter plots for each pair of some test scores dataset, and do sample skewness test.
```{r}
library(bootstrap)
library(ggplot2)
library(corrplot)
data("scor")
pairs(~mec+vec+alg+ana+sta,scor) ## scatter plots for each pairs of test scores
cor(scor) ## correlation matrix
corrplot(cor(scor),method="circle")

library(boot)
library(knitr)
set.seed(0)
## calculate the correlation
r12<-function(x,i){cor(x[i,1],scor[i,2])}
r34<-function(x,i){cor(x[i,3],scor[i,4])}
r35<-function(x,i){cor(x[i,3],scor[i,5])}
r45<-function(x,i){cor(x[i,4],scor[i,5])}

## bootstrap
obj12<-boot(data=scor,statistic = r12, R=2000)
obj34<-boot(data=scor,statistic = r34, R=2000)
obj35<-boot(data=scor,statistic = r35, R=2000)
obj45<-boot(data=scor,statistic = r45, R=2000)

## calculate and print the estimated standard errors
y12<-obj12$t;y34<-obj34$t;y35<-obj35$t;y45<-obj45$t
knitr::kable(cbind(sd(y12),sd(y34),sd(y35),sd(y45)),formate="html",col.names = c("std.error of r12","std.error of r34","std.error of r35","std.error of r45"))

n<-50
m<-1000
##cover probability
L1<-L2<-L3<-L4<-L5<-L6<-0  
## sample skewness coeff
sk<-function(x,ind){
xbar<-mean(x[ind])
m3<-mean((x-xbar)^3)
m2<-mean((x-xbar)^2)
return(m3/m2^1.5)
}
## confidence intervals for normal distribution
boot.obj<-boot(rnorm(n,0,1),statistic = sk,R=2000)
print(boot.ci(boot.obj,type=c("norm","basic","perc")))

## normal distribution(skewness 0) and normal CI coverage probability
for(i in 1:m){
x1<-rnorm(n,0,1)
boot.obj1<-boot(x1,statistic = sk, R=2000)
boot.ci(boot.obj1,type="norm")
ci<-data.frame(boot.ci(boot.obj1,type="norm")[4])
LCL<-as.numeric(ci[2]);UCL<-as.numeric(ci[3])
  if(0>=LCL&0<=UCL){
    L1<-L1+1
  }
}
L1/m

## normal distribution(skewness 0) and basic CI coverage probability
for(i in 1:m){
x1<-rnorm(n,0,1)
boot.obj1<-boot(x1,statistic = sk, R=2000)
boot.ci(boot.obj1,type="basic")
ci<-data.frame(boot.ci(boot.obj1,type="basic")[4])
LCL<-as.numeric(ci[4]);UCL<-as.numeric(ci[5])
  if(0>=LCL&0<=UCL){
    L2<-L2+1
  }
}
L2/m

## normal distribution(skewness 0) and percemtile CI coverage probability
for(i in 1:m){
x1<-rnorm(n,0,1)
boot.obj1<-boot(x1,statistic = sk, R=2000)
boot.ci(boot.obj1,type="perc")
ci<-data.frame(boot.ci(boot.obj1,type="perc")[4])
LCL<-as.numeric(ci[4]);UCL<-as.numeric(ci[5])
  if(0>=LCL&0<=UCL){
    L3<-L3+1
  }
}
L3/m

## confidence intervals for chisquare(5) distribution
boot.obj<-boot(rchisq(n,5),statistic = sk,R=2000)
print(boot.ci(boot.obj,type=c("norm","basic","perc")))

## chi-square(5) distribution and normal CI coverage probability
for(i in 1:m){
x2<-rchisq(n,5)
boot.obj1<-boot(x2,statistic = sk, R=2000)
boot.ci(boot.obj1,type="norm")
ci<-data.frame(boot.ci(boot.obj1,type="norm")[4])
LCL<-as.numeric(ci[2]);UCL<-as.numeric(ci[3])
  if(sqrt(8/5)>=LCL&sqrt(8/5)<=UCL){
    L4<-L4+1
  }
}
L4/m

## chi-square(5) distribution and basic CI coverage probability
for(i in 1:m){
x2<-rchisq(n,5)
boot.obj1<-boot(x2,statistic = sk, R=2000)
boot.ci(boot.obj1,type="basic")
ci<-data.frame(boot.ci(boot.obj1,type="basic")[4])
LCL<-as.numeric(ci[4]);UCL<-as.numeric(ci[5])
  if(sqrt(8/5)>=LCL&sqrt(8/5)<=UCL){
    L5<-L5+1
  }
}
L5/m

## chi-square(5) distribution and percentile CI coverage probability
for(i in 1:m){
x2<-rchisq(n,5)
boot.obj1<-boot(x2,statistic = sk, R=2000)
boot.ci(boot.obj1,type="perc")
ci<-data.frame(boot.ci(boot.obj1,type="perc")[4])
LCL<-as.numeric(ci[4]);UCL<-as.numeric(ci[5])
  if(sqrt(8/5)>=LCL&sqrt(8/5)<=UCL){
    L6<-L6+1
  }
}
L6/m

knitr::kable(cbind(L1/m,L2/m,L3/m,L4/m,L5/m,L6/m),formate="html",col.names=c("coverage rate for N(0,1)-normal","coverage rate for N(0,1)-basic","coverage rate for N(0,1)-perc","coverage rate for chisq(5)-normal","coverage rate for chisq(5)-basic","coverage rate for chisq(5)-perc"))
```

HW7: Obtain the jackknife estimates of bias and standard error and do cross validation.
```{r}
library(bootstrap)
library(knitr)
data(scor,package = "bootstrap")
## MLE
n<-nrow(scor)
cov.hat<-(n-1)/n*cov(scor)
lamda.hat<-eigen(cov.hat)$values
theta.hat<-lamda.hat[1]/sum(lamda.hat)

## jackknife
theta.jack<-numeric(n)
evalss<-matrix(nrow=88,ncol=5)
for(i in 1:n)
{
cc<-cov(scor[-i,])
evalss[i,]<- eigen(cc)$values  
theta.jack[i]<-evalss[i,1]/sum(evalss[i,])
}
##jackknife estimate of bias
bias <- (n-1)*(mean(theta.jack)-theta.hat)  
##jacknife estimate of standard deviation
se<-sqrt((n-1)*mean((theta.jack-mean(theta.jack))^2)) 

knitr::kable(cbind(theta.hat,bias,se),formate="html",col.names=c("theta.hat","jackknife estimate of bias","jackknife estimate of standard error"))

library(DAAG)
attach(ironslag)
#sequence for plotting fits
a <- seq(10, 40, .1) 
n <- length(magnetic) 
e1 <- e2 <- e3 <- e4 <- numeric(n)

# for n-fold cross validation
# fit models on leave-one-out samples 
for (k in 1:n) {
    y <- magnetic[-k]
    x <- chemical[-k]
# linear model
J1 <- lm(y ~ x)
yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k] 
e1[k] <- magnetic[k] - yhat1

# quadratic model
J2 <- lm(y ~ x + I(x^2))
yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +J2$coef[3] * chemical[k]^2
e2[k] <- magnetic[k] -yhat2

# exponential model
J3 <- lm(log(y) ~ x) 
logyhat3 <- J3$coef[1]++ J3$coef[2] * chemical[k]
yhat3 <- exp(logyhat3) 
e3[k] <- magnetic[k] -yhat3

# cubic polynomial model
J4 <- lm(y~x+I(x^2)+I(x^3))
yhat4 <- J4$coef[1] + J4$coef[2] * chemical[k] + J4$coef[3] * chemical[k]^2 +J4$coef[4] * chemical[k]^3
e4[k] <- magnetic[k] - yhat4
}

# display four plots
## par(mfrow=c(2,2))
L1<-lm(magnetic ~ chemical) 
plot(chemical, magnetic, main="Linear", pch=16) 
yhat1 <- L1$coef[1] + L1$coef[2] * a
lines(a, yhat1, lwd=2)

L2 <- lm(magnetic ~ chemical + I(chemical^2)) 
plot(chemical, magnetic, main="Quadratic", pch=16) 
yhat2 <- L2$coef[1] + L2$coef[2] * a + L2$ coef[3] * a^2
lines(a, yhat2, lwd=2)

L3 <- lm(log(magnetic) ~ chemical)
plot(chemical, magnetic, main="Exponential", pch=16)
logyhat3 <- L3$coef[1] + L3$coef[2] * a 
yhat3 <- exp(logyhat3) 
lines(a, yhat3, lwd=2)

L4 <- lm(magnetic ~ chemical + I(chemical^2)+I(chemical^3)) 
plot(chemical, magnetic, main="Cubic", pch=16) 
yhat4 <- L4$coef[1] + L4$coef[2] * a + L4$coef[3] * a^2+L4$coef[4] * a^3
lines(a, yhat2, lwd=2)

# model selection according to cross validation
knitr::kable(cbind(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2)),formate="html",col.names=c("prediction error for Model 1","prediction error for Model 2","prediction error for Model 3","prediction error for Model 4"))

# model selection according to adjusted R^2
knitr::kable(cbind(summary(L1)$adj.r.squared,summary(L2)$adj.r.squared,summary(L3)$adj.r.squared,summary(L4)$adj.r.squared),formate="html",col.names=c("AdjustedR^2 for L1","AdjustedR^2 for L2","AdjustedR^2 for L3","AdjustedR^2 for L4"))
```

HW8: Implement a permutation test for equal variance based on the maximum number of extreme points and model power comparision.
```{r}
count5test<-function(x,y){
  X<-x-mean(x)
  Y<-y-mean(y)
  outx<-sum(X>max(Y))+sum(X<min(Y))
  outy<-sum(Y>max(X))+sum(Y<min(X))
  ## return 1 (reject) or 0 (do not reject H0)
  return(as.integer(max(c(outx,outy))>5))
}

n1<-20
n2<-30
mu1<-mu2<-0
sigma1<-sigma2<-1
m<-1000

alphahat<-mean(replicate(m,expr={
  x<-rnorm(n1,mu1,sigma1)
  y<-rnorm(n2,mu2,sigma2)
  x<-x-mean(x)  ## centered by sample mean
  y<-y-mean(y)
  count5test(x,y)
}))
print(alphahat)

## permutation procedure
n1 <- 20
n2 <- 30
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1

set.seed(2345)
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)

# the maximum numebers of extreme points from the reference book
m1 <- 4
m2 <- 7

# original statistic
count <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  return(as.integer((outx > m1) | (outy > m2))) ## return 1 means rejection
}

R <- 999
z <- c(x,y)
K <- n1 + n2
reps <- numeric(R)
t0 <- count(x,y)
for (i in 1:R) {
  k <- sample(K, size = n1, replace = FALSE)
  x1 <- z[k]
  y1 <- z[-k]
  X <- x1 - mean(x1)
  Y <- y1 - mean(y1)
  reps[i] <- count(x1, y1)
}

# compute alphahat
alphahat <- mean(c(t0, reps) > t0)
print(alphahat)

library(MASS)
library(boot)
library(Ball)
set.seed(12345)
N<-100
n<-10
R<-999
mu1<-c(0,0)
mu2<-c(0,0)
sigma1<-matrix(c(1,0.8,0.8,2),nrow=2)
sigma2<-matrix(c(0.3,0.1,0.1,0.2),nrow=2)
alpha<-0.05
pvalue.dis<-pvalue.ball<-matrix(0,N,2)
## distance correlation test

## dCov function
dCov <- function(x, y) { 
  x <- as.matrix(x);  y <- as.matrix(y)
  n <- nrow(x); m <- nrow(y)
  if (n != m || n < 2) stop("Sample sizes must agree") 
  if (! (all(is.finite(c(x, y)))))
    stop("Data contains missing or infinite values")
  Akl <- function(x) {
    d <- as.matrix(dist(x))
    m <- rowMeans(d); M <- mean(d)
    a <- sweep(d, 1, m); b <- sweep(a, 2, m) 
    b + M
  }
  A <- Akl(x);  B <- Akl(y)
  sqrt(mean(A * B)) 
}

ndCov2 <- function(z, ix, dims) {
  #dims contains dimensions of x and y 
  p <- dims[1]
  q <- dims[2]
  d <- p + q
  x <- z[ , 1:p] #leave x as is
  y <- z[ix, -(1:p)] #permute rows of y 
  return(nrow(z) * dCov(x, y)^2)
}

 ## Monte Carlo
for (i in 1:N) {
  X <- mvrnorm(n,mu1,sigma1)
  e <- mvrnorm(n,mu2,sigma2)
  Y1 <- X/4 + e
  Y2 <- X/4 * e
  z1 <- cbind(X, Y1)
  z2 <- cbind(X, Y2)
  boot.obj1 <- boot(data = z1, statistic = ndCov2, R = R, sim = 'permutation', dims = c(2,2))
  tb1 <- c(boot.obj1$t0, boot.obj1$t)
  boot.obj2 <- boot(data = z2, statistic = ndCov2, R = R, sim = 'permutation', dims = c(2,2))
  tb2 <- c(boot.obj2$t0, boot.obj2$t)
  pvalue.dis[i, 1] <- mean(tb1 >= tb1[1])
  pvalue.dis[i, 2] <- mean(tb2 >= tb2[1])
  pvalue.ball[i, 1] <- bd.test(X, Y1, R = 999, seed = i )$p.value
  pvalue.ball[i, 2] <- bd.test(X, Y2, R = 999, seed = i )$p.value 
}

pwr1.n10 <- colMeans(pvalue.dis < alpha)
pwr2.n10 <- colMeans(pvalue.ball < alpha)
pwr1.n10
pwr2.n10

```

HW9: Implement a random walk Metropolis sampler for generating the standard Laplace distribution.
```{r}
library(knitr)
set.seed(1)
rw.Metropolis<-function(sigma,x0,N){
  x<-numeric(N)
  x[1]<-x0
  u<-runif(N)
  k<-0
  for (i in 2:N){
    y<-rnorm(1,x[i-1],sigma)
    if(u[i]<= exp(abs(x[i-1])-abs(y)))
      {x[i]<-y }
    else{
        x[i]<-x[i-1]
        k<-k+1
      }
  }
return(list(x=x,k=k))
}

N<-2000
sigma<-c(.05,.5,2,16)
x0<-25
rw1<-rw.Metropolis(sigma[1],x0,N)
rw2<-rw.Metropolis(sigma[2],x0,N)
rw3<-rw.Metropolis(sigma[3],x0,N)
rw4<-rw.Metropolis(sigma[4],x0,N)
#number of candidate points rejected
knitr::kable(cbind(rw1$k/2000,rw2$k/2000,rw3$k/2000,rw4$k/2000),col.names=c("rejection rate of sigma=0.05","arejection rate of sigma=0.5","rejection rate of sigma=2","rejection rate of sigma=16"))
##number of candidate points accepted
knitr::kable(cbind((2000-rw1$k)/2000,(2000-rw2$k)/2000,(2000-rw3$k)/2000,(2000-rw4$k)/2000),col.names=c("acceptance rate of sigma=0.05","acceptance rate of sigma=0.5","acceptance rate of sigma=2","acceptance rate of sigma=16"))
```

HW10: Show by example that the natural logarithm and exponential functions are inverse of each othe does not hold exactly in computer arithmetic, write a function to solve an equation, and A-B-O blood type problem.
```{r,warning=FALSE}
set.seed(12)
n<-10
x0<-runif(n,0,1)
x1<-log(exp(x0))
x2<-exp(log(x0))

x0==x1
x0==x2
x1==x2
## using all.equal to check for near equality
isTRUE(all.equal(x0,x1))
isTRUE(all.equal(x0,x2))
isTRUE(all.equal(x1,x2))

k0<-4
f <- function(a, k0) 1-pt(sqrt(a^2*k0/(k0+1-a^2)), k0)
g<-function(x) f(x,k0-1)-f(x,k0)
a<-seq(0,sqrt(k0),0.1)
plot(a,g(a),type='l')

k <- c(4:25,100,500,1000)
f <- function(a, k) 1-pt(sqrt(a^2*k/(k+1-a^2)), k)
root1<-numeric(length(k))
for (i in 1:25){
  g<-function(x) f(x,k[i]-1)-f(x,k[i])
  root1[i]<-uniroot(g, lower = 1, upper = 2)$root
}

library(knitr)
root2<-numeric(length(k))

for(i in 1:25){
  g1<-function(u)  (1+u^2/(k[i]-1))^(-k[i]/2)
  
g2<-function(u) (1+u^2/k[i])^(-(k[i]+1)/2)

C<-function(k0,a) sqrt(a^2*k0/(k0+1-a^2))

f<-function(a){
2*exp(lgamma(k[i]/2)-lgamma((k[i]-1)/2))/sqrt(pi*(k[i]-1))*integrate(g1,0,C(k[i]-1,a))$value-2*exp(lgamma((k[i]+1)/2)-lgamma(k[i]/2))/sqrt(pi*k[i])*integrate(g2,0,C(k[i],a))$value
}
  root2[i]<-uniroot(f,lower=1, upper=2)$root
}

knitr::kable(cbind(root1,root2),formate="html",col.names=c("intersection points A(k)","root"))

theta0<-c(0.3,0.5)
l<-numeric(1000)
for (i in 1:1000){
LL<-function(theta){
  p<-theta[1]
  q<-theta[2]
  r<-1-p-q
  p0<-theta0[1]
  q0<-theta0[2]
  r0<-1-theta0[1]-theta0[2]
  x<-28*p0/(p0+2*r0)
  y<-24*q0/(q0+2*r0)
  loglik<-(2*x)*log(p)+(2*y)*log(q)+(2*41)*log(r)+(28-x)*log(2*p*r)+(24-y)*log(2*q*r)+70*log(2*p*q)
  -loglik
}
optim<-optim(c(0.4,0.3),LL)
theta0<-optim$par
l[i]<-LL(theta0)
}

theta0

a<-seq(1,20,by=1)
plot(a,-l[a],type='l',xlab="M-steps",ylab="log-maximum likelihood values")
```

HW11: Use function $apply()$ and his friends to solve problems.
```{r}
attach(mtcars)

formulas = list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)
#1 for loops
f3 = vector("list", length(formulas))
for (i in seq_along(formulas)){
  f3[[i]] = lm(formulas[[i]], data = mtcars)
}
f3
#2 lapply
la3 = lapply(formulas, function(x) lm(formula = x, data = mtcars))
la3

bootstraps = lapply(1:10, function(i) {
  rows = sample(1:nrow(mtcars), rep = TRUE)
  mtcars[rows, ]
})
# for loops
f4 = vector("list", length(bootstraps))
for (i in seq_along(bootstraps)){
  f4[[i]] = lm(mpg ~ disp, data = bootstraps[[i]])
}
f4
# lapply without anonymous function
la4 = lapply(bootstraps, lm, formula = mpg ~ disp)
la4

rsq = function(mod) summary(mod)$r.squared
#3
sapply(la3, rsq)
sapply(f3, rsq)
#4
sapply(la4,rsq)
sapply(f4,rsq)

trials = replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
# anonymous function:
sapply(trials, function(x) x[["p.value"]])

library(parallel)

num_cores = detectCores()
cluster = makePSOCKcluster(num_cores)

mcsapply = function(cluster,X,FUN,...){
 res = parLapply(cluster,X,FUN,...) 
 simplify2array(res)
}

system.time(mcsapply(cluster, 1:10, function(i) Sys.sleep(1)))
system.time(sapply(1:10, function(i) Sys.sleep(1)))

stopCluster(cluster)

```

HW12: Write Rcpp function with R function and compare there system time to see how much time Rcpp has saved.
```{r}
library(SC19030)
library(Rcpp)
library(knitr)
library(microbenchmark)
N<-2000
sigma<-c(.05,.5,2,16)
x0<-25


rw1<-rwMetropolisC(sigma[1],x0,N)
rw2<-rwMetropolisC(sigma[2],x0,N)
rw3<-rwMetropolisC(sigma[3],x0,N)
rw4<-rwMetropolisC(sigma[4],x0,N)
k1<-rw1[N+1];k2<-rw2[N+1];k3<-rw3[N+1];k4<-rw4[N+1]

#number of candidate points rejected
knitr::kable(cbind(k1/2000,k2/2000,k3/2000,k4/2000),col.names=c("rejection rate of sigma=0.05","arejection rate of sigma=0.5","rejection rate of sigma=2","rejection rate of sigma=16"))
##number of candidate points accepted
knitr::kable(cbind((2000-k1)/2000,(2000-k2)/2000,(2000-k3)/2000,(2000-k4)/2000),col.names=c("acceptance rate of sigma=0.05","acceptance rate of sigma=0.5","acceptance rate of sigma=2","acceptance rate of sigma=16"))

```

```{r}
set.seed(1)
rw.Metropolis<-function(sigma,x0,N){
  x<-numeric(N)
  x[1]<-x0
  u<-runif(N)
  k<-0
  for (i in 2:N){
    y<-rnorm(1,x[i-1],sigma)
    if(u[i]<= exp(abs(x[i-1])-abs(y)))
      {x[i]<-y }
    else{
        x[i]<-x[i-1]
        k<-k+1
      }
  }
return(list(x=x,k=k))
}

rw5<-rw.Metropolis(sigma[1],x0,N)
rw6<-rw.Metropolis(sigma[2],x0,N)
rw7<-rw.Metropolis(sigma[3],x0,N)
rw8<-rw.Metropolis(sigma[4],x0,N)
#number of candidate points rejected
knitr::kable(cbind(rw5$k/2000,rw6$k/2000,rw7$k/2000,rw8$k/2000),col.names=c("rejection rate of sigma=0.05","arejection rate of sigma=0.5","rejection rate of sigma=2","rejection rate of sigma=16"))
##number of candidate points accepted
knitr::kable(cbind((2000-rw5$k)/2000,(2000-rw6$k)/2000,(2000-rw7$k)/2000,(2000-rw8$k)/2000),col.names=c("acceptance rate of sigma=0.05","acceptance rate of sigma=0.5","acceptance rate of sigma=2","acceptance rate of sigma=16"))
```

```{r}
rw1<-rw1[-(N+1)];rw2<-rw2[-(N+1)];rw3<-rw3[-(N+1)];rw4<-rw4[-(N+1)]
## par(mfcol = c(2, 2))
qqplot(rw1,rw5$x,xlab="rw with Cpp", ylab="rw with R")
qqplot(rw2,rw6$x,xlab="rw with Cpp", ylab="rw with R")
qqplot(rw3,rw7$x,xlab="rw with Cpp", ylab="rw with R")
qqplot(rw4,rw8$x,xlab="rw with Cpp", ylab="rw with R")
```

```{r}
ts<-microbenchmark(rw1=rwMetropolisC(sigma[1],x0,N),rw2=rwMetropolisC(sigma[2],x0,N),rw3=rwMetropolisC(sigma[3],x0,N),rw4=rwMetropolisC(sigma[4],x0,N),rw5=rw.Metropolis(sigma[1],x0,N),rw6=rw.Metropolis(sigma[2],x0,N),rw7=rw.Metropolis(sigma[3],x0,N),rw8=rw.Metropolis(sigma[4],x0,N))
knitr::kable(summary(ts)[,c(1,3,5,6)])
```